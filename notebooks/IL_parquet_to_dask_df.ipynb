{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import boto3\n",
    "import os\n",
    "import io\n",
    "import dask.dataframe as dd\n",
    "import s3fs\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up credentials in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your access key credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"xxxx\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"xxxx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test readubg single pandas file to dask array\n",
    "%time\n",
    "s3 = boto3.client('s3')\n",
    "bucket='datathon-2018'\n",
    "key='raw/MelbDatathon2018/Samp_0/ScanOnTransaction/2015/Week27/QID3530815_20180713_20515_0.txt.gz'\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "df = pd.read_csv(io.BytesIO(obj['Body'].read()),compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) #368226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests reading single parquet file to dask dataframe \n",
    "%time\n",
    "df=dd.read_parquet('s3://datathon-2018/parquet/taps/2015_week27.parquet')\n",
    "len(df) #1,287,268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reading about 10 parquet files in to dask dataframe\n",
    "df=dd.read_parquet('s3://datathon-2018/parquet/taps/2015_week2*.parquet')\n",
    "len(df) # 5,666,495\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reading in all files to dask array\n",
    "df=dd.read_parquet('s3://datathon-2018/parquet/taps/*.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.groupby(df.card_type).size().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# persist dataframe to cluster \n",
    "df = df.persist()\n",
    "progress(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in  dataframe from parquet files in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test file\n",
    "bucket_name='datathon-2018'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket=s3.Bucket(bucket_name)\n",
    "file_list_on=[]\n",
    "file_list=[obj.key for obj in bucket.objects.filter(Prefix='parquet/taps') if obj.key.endswith(\".parquet\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bucket_name='s3://datathon-2018/'\n",
    "df_taps = pd.concat([pd.read_parquet(bucket_name+f) for f in file_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_taps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
